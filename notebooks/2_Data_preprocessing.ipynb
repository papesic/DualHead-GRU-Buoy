{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "376a213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pesic\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Config ===\n",
      "Main: M2, Extra: ['M3'], Gen: M5\n",
      "SEQ_LEN=72, HORIZON=6, STRIDE=3\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 1 — Imports & Config (linked to Exploration Notebook outputs)\n",
    "# ================================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Load selected stations from exploration notebook output\n",
    "REPORTS_DIR = ROOT / \"reports\"\n",
    "selection_file = REPORTS_DIR / \"selected_buoys.json\"\n",
    "if not selection_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Selection file not found at {selection_file}. \"\n",
    "        \"Run the exploration notebook first.\"\n",
    "    )\n",
    "\n",
    "with open(selection_file, \"r\") as f:\n",
    "    selection = json.load(f)\n",
    "\n",
    "MAIN_STATION = selection[\"main_buoy\"]\n",
    "EXTRA_TRAIN_STATIONS = selection.get(\"extra_train_buoys\", [])\n",
    "GEN_STATION = selection[\"generalization_buoy\"]\n",
    "\n",
    "# Domain config\n",
    "SEQ_LEN = 72\n",
    "HORIZON = 6\n",
    "STRIDE = 3\n",
    "MAX_INTERP_GAP = 12\n",
    "K_DURATION = 2\n",
    "THRESH_MODE = \"seasonal_q90\"  # keep in physical units (meters)\n",
    "\n",
    "# Columns\n",
    "SENSOR_COLS = [\n",
    "    'AtmosphericPressure', 'WindDirection', 'WindSpeed', 'Gust',\n",
    "    'WaveHeight', 'WavePeriod', 'MeanWaveDirection', 'Hmax',\n",
    "    'AirTemperature', 'DewPoint', 'SeaTemperature', 'RelativeHumidity'\n",
    "]\n",
    "TIME_FEATS = ['HOUR_sin', 'HOUR_cos', 'WEEKDAY_sin', 'WEEKDAY_cos']\n",
    "PLACEHOLDERS = {\"NaN\", \"-999\", -999, -999.0}\n",
    "\n",
    "\n",
    "# Consistency for label unit fix later\n",
    "# keep a raw copy of WaveHeight for threshold calculations.\n",
    "# Scaling done after train/val/test split to prevent leakage.\n",
    "RAW_WAVEHEIGHT_COL = 'WaveHeight_raw'\n",
    "\n",
    "print(\"=== Config ===\")\n",
    "print(f\"Main: {MAIN_STATION}, Extra: {EXTRA_TRAIN_STATIONS}, Gen: {GEN_STATION}\")\n",
    "print(f\"SEQ_LEN={SEQ_LEN}, HORIZON={HORIZON}, STRIDE={STRIDE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8560aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TRAIN domain (['M2', 'M3']): 231519 rows from 2001-05-03 14:00:00+00:00 to 2017-11-28 10:00:00+00:00\n",
      "Loaded GEN buoy (M5): 101137 rows from 2004-10-18 10:00:00+00:00 to 2017-11-28 10:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 2 — Load & Filter (main + extras + gen) \n",
    "# ========================================\n",
    "RAW_CSV = DATA_DIR / \"Buoy_raw.csv\"\n",
    "if not RAW_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {RAW_CSV}\")\n",
    "\n",
    "# Load dataset\n",
    "df_raw = pd.read_csv(RAW_CSV, low_memory=False)\n",
    "\n",
    "# Drop the units row if present\n",
    "df_raw = df_raw[df_raw['station_id'] != 'station_id']\n",
    "df_raw = df_raw[df_raw['station_id'].notna()]\n",
    "\n",
    "# Ensure numeric for all sensor cols\n",
    "for col in SENSOR_COLS:\n",
    "    if col in df_raw.columns:\n",
    "        df_raw[col] = pd.to_numeric(df_raw[col], errors='coerce')\n",
    "\n",
    "# Convert time to datetime\n",
    "df_raw['time'] = pd.to_datetime(df_raw['time'], errors='coerce', utc=True)\n",
    "\n",
    "# Replace placeholders with NaN\n",
    "for col in SENSOR_COLS:\n",
    "    if col in df_raw.columns:\n",
    "        df_raw[col] = df_raw[col].replace(list(PLACEHOLDERS), np.nan)\n",
    "\n",
    "# Preserve raw WaveHeight for label computation\n",
    "RAW_WAVEHEIGHT_COL = \"WaveHeight_raw\"\n",
    "if 'WaveHeight' not in df_raw.columns:\n",
    "    raise KeyError(\"WaveHeight column not found in dataset.\")\n",
    "df_raw[RAW_WAVEHEIGHT_COL] = df_raw['WaveHeight'].copy()\n",
    "\n",
    "# Remove location leakage — drop lon/lat entirely\n",
    "if 'longitude' in df_raw.columns:\n",
    "    df_raw.drop(columns=['longitude'], inplace=True)\n",
    "if 'latitude' in df_raw.columns:\n",
    "    df_raw.drop(columns=['latitude'], inplace=True)\n",
    "\n",
    "# Encode cyclic wind direction features (replaces raw angles)\n",
    "for dir_col in ['WindDirection', 'MeanWaveDirection']:\n",
    "    if dir_col in df_raw.columns:\n",
    "        radians = np.deg2rad(df_raw[dir_col] % 360)\n",
    "        df_raw[f\"{dir_col}_sin\"] = np.sin(radians)\n",
    "        df_raw[f\"{dir_col}_cos\"] = np.cos(radians)\n",
    "        df_raw.drop(columns=[dir_col], inplace=True)\n",
    "\n",
    "# Update SENSOR_COLS\n",
    "SENSOR_COLS = [\n",
    "    c for c in SENSOR_COLS + ['WindDirection_sin', 'WindDirection_cos',\n",
    "                              'MeanWaveDirection_sin', 'MeanWaveDirection_cos']\n",
    "    if c in df_raw.columns and c not in ['WindDirection', 'MeanWaveDirection']\n",
    "]\n",
    "\n",
    "# Filter main + extra stations for training\n",
    "train_ids = [MAIN_STATION] + EXTRA_TRAIN_STATIONS\n",
    "df_train_raw = df_raw[df_raw['station_id'].isin(train_ids)].copy()\n",
    "\n",
    "# Filter generalization station separately\n",
    "df_gen_raw = df_raw[df_raw['station_id'] == GEN_STATION].copy()\n",
    "\n",
    "print(f\"Loaded TRAIN domain ({train_ids}): {len(df_train_raw)} rows \"\n",
    "      f\"from {df_train_raw['time'].min()} to {df_train_raw['time'].max()}\")\n",
    "print(f\"Loaded GEN buoy ({GEN_STATION}): {len(df_gen_raw)} rows \"\n",
    "      f\"from {df_gen_raw['time'].min()} to {df_gen_raw['time'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae6fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 3 — Clean (preserve raw WH), compute masks/deltas before fill,\n",
    "#           relative WH in meters, thresholds in meters.\n",
    "# ================================================================\n",
    "def compute_thresholds(df, mode=\"seasonal_q90\", raw_col=RAW_WAVEHEIGHT_COL):\n",
    "    \"\"\"Compute monthly or global 90th percentile thresholds from raw WH (meters).\"\"\"\n",
    "    if raw_col not in df.columns:\n",
    "        raise KeyError(f\"{raw_col} not found for threshold computation.\")\n",
    "    if mode == \"seasonal_q90\":\n",
    "        return df.groupby(df.index.month)[raw_col].quantile(0.90)\n",
    "    elif mode == \"global_q90\":\n",
    "        thr = df[raw_col].quantile(0.90)\n",
    "        return pd.Series([thr] * 12, index=range(1, 13))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown threshold mode: {mode}\")\n",
    "\n",
    "def clean_and_engineer(df, sensor_cols=SENSOR_COLS):\n",
    "    # Sort and drop duplicate timestamps\n",
    "    df = df.sort_values('time').drop_duplicates('time')\n",
    "\n",
    "    # Keep metadata\n",
    "    meta_cols = [c for c in ['station_id'] if c in df.columns]  # lon/lat removed\n",
    "    meta_data = df[meta_cols].iloc[0]\n",
    "\n",
    "    # Reindex to full hourly range\n",
    "    full_idx = pd.date_range(df['time'].min(), df['time'].max(), freq='H', tz='UTC')\n",
    "    df = df.set_index('time').reindex(full_idx)\n",
    "\n",
    "    # Restore metadata\n",
    "    for col in meta_cols:\n",
    "        df[col] = meta_data[col]\n",
    "\n",
    "    # Masks & deltas BEFORE filling\n",
    "    \n",
    "    masks = df[sensor_cols].isna().astype(np.int8)\n",
    "    deltas = pd.DataFrame(index=df.index)\n",
    "    for col in sensor_cols:\n",
    "        count = 0\n",
    "        delta_col = []\n",
    "        for missing in masks[col]:\n",
    "            count = 0 if missing == 0 else count + 1\n",
    "            delta_col.append(count)\n",
    "        deltas[col] = delta_col\n",
    "    masks.columns = [f\"{c}_mask\" for c in masks.columns]\n",
    "    deltas.columns = [f\"{c}_delta\" for c in deltas.columns]\n",
    "\n",
    "    \n",
    "    # Interpolation & seasonal median fill\n",
    "    \n",
    "    for col in sensor_cols:\n",
    "        df[col] = df[col].interpolate(method='time', limit=MAX_INTERP_GAP, limit_direction='both')\n",
    "        month_median = df[col].groupby(df.index.month).transform('median')\n",
    "        df[col] = df[col].fillna(month_median)\n",
    "\n",
    "    \n",
    "    # Compute thresholds in meters from WaveHeight_raw\n",
    "    \n",
    "    thresholds = compute_thresholds(df, mode=THRESH_MODE, raw_col=RAW_WAVEHEIGHT_COL)\n",
    "\n",
    "    # Relative WH in meters (unscaled for now)\n",
    "    month_series = df.index.month\n",
    "    df[\"WaveHeight_rel\"] = (df[RAW_WAVEHEIGHT_COL] - month_series.map(thresholds)) / month_series.map(thresholds)\n",
    "    df[\"WaveHeight_rel\"] = df[\"WaveHeight_rel\"].fillna(0)\n",
    "\n",
    "    # Replace any residual NaNs in sensor cols (before scaling)\n",
    "    for col in sensor_cols:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    \n",
    "    # Time features\n",
    "    df['HOUR_sin'] = np.sin(2 * np.pi * df.index.hour / 24)\n",
    "    df['HOUR_cos'] = np.cos(2 * np.pi * df.index.hour / 24)\n",
    "    df['WEEKDAY_sin'] = np.sin(2 * np.pi * df.index.weekday / 7)\n",
    "    df['WEEKDAY_cos'] = np.cos(2 * np.pi * df.index.weekday / 7)\n",
    "\n",
    "    \n",
    "    # Attach masks & deltas\n",
    "    \n",
    "    df = pd.concat([df, masks, deltas], axis=1)\n",
    "\n",
    "    return df, thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b0aed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned M2 → C:\\Users\\pesic\\Desktop\\GRU\\data\\processed\\clean_M2.npz (145269 hours, cols=50)\n",
      "Saved cleaned M3 → C:\\Users\\pesic\\Desktop\\GRU\\data\\processed\\clean_M3.npz (130958 hours, cols=50)\n",
      "Saved cleaned M5 → C:\\Users\\pesic\\Desktop\\GRU\\data\\processed\\clean_M5.npz (114937 hours, cols=50)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  4 — Clean all training buoys & generalization buoy\n",
    "#   Preserve WaveHeight_raw and clean NaNs/Inf\n",
    "# ================================================================\n",
    "\n",
    "def save_clean_npz(df, station_id, thresholds):\n",
    "    \"\"\"Save cleaned dataframe + thresholds for one buoy.\"\"\"\n",
    "    # Ensure no column ordering issues\n",
    "    df = df.copy()\n",
    "    if RAW_WAVEHEIGHT_COL not in df.columns:\n",
    "        raise KeyError(f\"{RAW_WAVEHEIGHT_COL} missing in cleaned dataframe for {station_id}\")\n",
    "\n",
    "    # Final NaN/Inf check — keep but fill with 0 for safe storage\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    path = PROCESSED_DIR / f\"clean_{station_id}.npz\"\n",
    "    np.savez_compressed(\n",
    "        path,\n",
    "        data=df.to_numpy(),\n",
    "        index=df.index.astype(str).to_numpy(),\n",
    "        columns=df.columns.to_numpy(),\n",
    "        thresholds=thresholds.to_numpy(),\n",
    "        threshold_months=thresholds.index.to_numpy()\n",
    "    )\n",
    "    print(f\"Saved cleaned {station_id} → {path} ({len(df)} hours, cols={len(df.columns)})\")\n",
    "\n",
    "def load_clean_npz(path):\n",
    "    \"\"\"Load cleaned dataframe + thresholds from disk.\"\"\"\n",
    "    npz = np.load(path, allow_pickle=True)\n",
    "    df = pd.DataFrame(\n",
    "        data=npz[\"data\"],\n",
    "        index=pd.to_datetime(npz[\"index\"], utc=True),\n",
    "        columns=npz[\"columns\"]\n",
    "    )\n",
    "    thresholds = pd.Series(npz[\"thresholds\"], index=npz[\"threshold_months\"])\n",
    "    return df, thresholds\n",
    "\n",
    "# Clean and save each training buoy separately\n",
    "\n",
    "df_train_clean_list = []\n",
    "for sid in train_ids:\n",
    "    df_buoy_clean, thr_buoy = clean_and_engineer(\n",
    "        df_train_raw[df_train_raw['station_id'] == sid]\n",
    "    )\n",
    "    df_buoy_clean.index = pd.to_datetime(df_buoy_clean.index, utc=True)\n",
    "    save_clean_npz(df_buoy_clean, sid, thr_buoy)\n",
    "    df_train_clean_list.append(df_buoy_clean)\n",
    "\n",
    "# Merge cleaned training buoys (for diagnostics only)\n",
    "df_train_clean = pd.concat(df_train_clean_list).sort_index()\n",
    "\n",
    "# Clean & save generalization buoy\n",
    "\n",
    "df_gen_clean, thr_gen = clean_and_engineer(df_gen_raw)\n",
    "df_gen_clean.index = pd.to_datetime(df_gen_clean.index, utc=True)\n",
    "save_clean_npz(df_gen_clean, GEN_STATION, thr_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52170129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train domain sequences: (92024, 72, 47), Danger%=9.22\n",
      "Gen sequences: (38287, 72, 47), Danger%=8.05\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 5 — Sequence generation\n",
    "# ================================================================\n",
    "def make_sequences(df_b, thresholds, seq_len=SEQ_LEN, horizon=HORIZON, stride=STRIDE):\n",
    "    \"\"\"Create sliding window sequences with danger labels in meters.\"\"\"\n",
    "    mask_cols = [c for c in df_b.columns if c.endswith('_mask')]\n",
    "    delta_cols = [c for c in df_b.columns if c.endswith('_delta')]\n",
    "    feature_cols = SENSOR_COLS + ['WaveHeight_rel'] + mask_cols + delta_cols + TIME_FEATS\n",
    "\n",
    "    X_seq, y_cls, y_reg, dates_seq = [], [], [], []\n",
    "\n",
    "    for i in range(0, len(df_b) - seq_len - horizon, stride):\n",
    "        X_window = df_b.iloc[i:i+seq_len][feature_cols].values\n",
    "\n",
    "        # Use raw WH (meters) for labels\n",
    "        future_wvht_raw = df_b.iloc[i+seq_len:i+seq_len+horizon][RAW_WAVEHEIGHT_COL].values\n",
    "        month = df_b.index[i+seq_len].month\n",
    "        thresh = thresholds.loc[month]\n",
    "        if np.isnan(thresh):\n",
    "            continue\n",
    "\n",
    "        danger = int(any(\n",
    "            np.convolve((future_wvht_raw > thresh).astype(int),\n",
    "                        np.ones(K_DURATION, dtype=int),\n",
    "                        'valid') >= K_DURATION\n",
    "        ))\n",
    "\n",
    "        y_cls.append(danger)\n",
    "        y_reg.append(future_wvht_raw.mean())  # regression target in meters\n",
    "        X_seq.append(X_window)\n",
    "        dates_seq.append(df_b.index[i+seq_len])\n",
    "\n",
    "    return (\n",
    "        np.array(X_seq, dtype=np.float32),\n",
    "        np.array(y_cls, dtype=np.int64),\n",
    "        np.array(y_reg, dtype=np.float32),\n",
    "        pd.DatetimeIndex(dates_seq)\n",
    "    )\n",
    "\n",
    "# Generate sequences for each buoy in training domain\n",
    "X_train_all, y_train_cls_all, y_train_reg_all, dates_train_all = [], [], [], []\n",
    "for sid in train_ids:\n",
    "    df_b, thr_b = load_clean_npz(PROCESSED_DIR / f\"clean_{sid}.npz\")\n",
    "    X_seq, y_cls, y_reg, dates_seq = make_sequences(df_b, thr_b)\n",
    "    X_train_all.append(X_seq)\n",
    "    y_train_cls_all.append(y_cls)\n",
    "    y_train_reg_all.append(y_reg)\n",
    "    dates_train_all.append(dates_seq)\n",
    "\n",
    "# Merge all training sequences\n",
    "X_main = np.vstack(X_train_all)\n",
    "y_main_cls = np.hstack(y_train_cls_all)\n",
    "y_main_reg = np.hstack(y_train_reg_all)\n",
    "dates_main = pd.DatetimeIndex(np.hstack(dates_train_all))\n",
    "\n",
    "# Generalization buoy\n",
    "\n",
    "df_gen_b, thr_gen = load_clean_npz(PROCESSED_DIR / f\"clean_{GEN_STATION}.npz\")\n",
    "X_gen, y_gen_cls, y_gen_reg, dates_gen = make_sequences(df_gen_b, thr_gen)\n",
    "\n",
    "print(f\"Train domain sequences: {X_main.shape}, Danger%={100*y_main_cls.mean():.2f}\")\n",
    "print(f\"Gen sequences: {X_gen.shape}, Danger%={100*y_gen_cls.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7860ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overlaps between Train, Val, and Test splits.\n",
      "Train: All samples finite (64416 total)\n",
      "Val: All samples finite (13804 total)\n",
      "Test: All samples finite (13804 total)\n",
      "Gen: All samples finite (38287 total)\n",
      "All features and targets are finite after cleaning.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 6 — Chronological split with strict non-overlapping boundaries\n",
    "# ================================================================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 1. Chronological split by percentage\n",
    "# -----------------\n",
    "p = pd.Series(dates_main).rank(pct=True).values\n",
    "train_mask = p < 0.70\n",
    "val_mask   = (p >= 0.70) & (p < 0.85)\n",
    "test_mask  = p >= 0.85\n",
    "\n",
    "X_train, y_train_cls, y_train_reg, dates_train = (\n",
    "    X_main[train_mask], y_main_cls[train_mask], y_main_reg[train_mask], dates_main[train_mask]\n",
    ")\n",
    "X_val, y_val_cls, y_val_reg, dates_val = (\n",
    "    X_main[val_mask], y_main_cls[val_mask], y_main_reg[val_mask], dates_main[val_mask]\n",
    ")\n",
    "X_test, y_test_cls, y_test_reg, dates_test = (\n",
    "    X_main[test_mask], y_main_cls[test_mask], y_main_reg[test_mask], dates_main[test_mask]\n",
    ")\n",
    "\n",
    "# 2. Station-aware overlap check\n",
    "# -----------------\n",
    "def remove_overlap(dates_a, dates_b, X_a, y_a_cls, y_a_reg, stations_a=None, stations_b=None):\n",
    "    if stations_a is not None and stations_b is not None:\n",
    "        tuples_a = list(zip(stations_a, dates_a))\n",
    "        tuples_b = set(zip(stations_b, dates_b))\n",
    "        overlap_mask = [t in tuples_b for t in tuples_a]\n",
    "    else:\n",
    "        overlap_mask = pd.Series(dates_a).isin(pd.Series(dates_b))\n",
    "    if any(overlap_mask):\n",
    "        print(f\"Removing {sum(overlap_mask)} overlapping samples from split.\")\n",
    "        keep_mask = ~pd.Series(overlap_mask)\n",
    "        return dates_a[keep_mask], X_a[keep_mask], y_a_cls[keep_mask], y_a_reg[keep_mask]\n",
    "    return dates_a, X_a, y_a_cls, y_a_reg\n",
    "\n",
    "dates_val, X_val, y_val_cls, y_val_reg = remove_overlap(dates_val, dates_test, X_val, y_val_cls, y_val_reg)\n",
    "\n",
    "\n",
    "# 3. Final overlap check\n",
    "# -----------------\n",
    "train_val_ov  = pd.Series(dates_train).isin(dates_val).sum()\n",
    "val_test_ov   = pd.Series(dates_val).isin(dates_test).sum()\n",
    "train_test_ov = pd.Series(dates_train).isin(dates_test).sum()\n",
    "\n",
    "if any(x > 0 for x in [train_val_ov, val_test_ov, train_test_ov]):\n",
    "    raise ValueError(f\"Overlap remains after cleaning: Train–Val={train_val_ov}, Val–Test={val_test_ov}, Train–Test={train_test_ov}\")\n",
    "else:\n",
    "    print(\"No overlaps between Train, Val, and Test splits.\")\n",
    "\n",
    "# 4. Alignment + NaN/Inf cleaning for each split (dates included)\n",
    "# -----------------\n",
    "def align_and_clean(X, y_cls, y_reg, dates, split_name):\n",
    "    # Align by min length\n",
    "    min_len = min(len(X), len(y_cls), len(y_reg), len(dates))\n",
    "    if not (len(X) == len(y_cls) == len(y_reg) == len(dates)):\n",
    "        print(f\"{split_name}: Mismatch in lengths -> trimming to {min_len} samples\")\n",
    "        X, y_cls, y_reg, dates = X[:min_len], y_cls[:min_len], y_reg[:min_len], dates[:min_len]\n",
    "    # Clean NaN/Inf\n",
    "    mask = (\n",
    "        np.isfinite(X).all(axis=(1, 2)) &\n",
    "        np.isfinite(y_cls) &\n",
    "        np.isfinite(y_reg)\n",
    "    )\n",
    "    removed = np.count_nonzero(~mask)\n",
    "    kept = np.count_nonzero(mask)\n",
    "    if removed > 0:\n",
    "        print(f\"{split_name}: Removed {removed} / {len(mask)} samples (kept {kept}) due to non-finite values\")\n",
    "    else:\n",
    "        print(f\"{split_name}: All samples finite ({kept} total)\")\n",
    "    return X[mask], y_cls[mask], y_reg[mask], dates[mask]\n",
    "\n",
    "X_train, y_train_cls, y_train_reg, dates_train = align_and_clean(X_train, y_train_cls, y_train_reg, dates_train, \"Train\")\n",
    "X_val,   y_val_cls,   y_val_reg,   dates_val   = align_and_clean(X_val,   y_val_cls,   y_val_reg,   dates_val,   \"Val\")\n",
    "X_test,  y_test_cls,  y_test_reg,  dates_test  = align_and_clean(X_test,  y_test_cls,  y_test_reg,  dates_test,  \"Test\")\n",
    "X_gen,   y_gen_cls,   y_gen_reg,   dates_gen   = align_and_clean(X_gen,   y_gen_cls,   y_gen_reg,   dates_gen,   \"Gen\")\n",
    "\n",
    "# 5. Train-only scaling\n",
    "# -----------------\n",
    "num_features = X_train.shape[2]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, num_features)).reshape(X_train.shape)\n",
    "X_val_scaled   = scaler.transform(X_val.reshape(-1, num_features)).reshape(X_val.shape)\n",
    "X_test_scaled  = scaler.transform(X_test.reshape(-1, num_features)).reshape(X_test.shape)\n",
    "X_gen_scaled   = scaler.transform(X_gen.reshape(-1, num_features)).reshape(X_gen.shape)\n",
    "\n",
    "# 6. Final verification\n",
    "# -----------------\n",
    "for name, arr in [\n",
    "    (\"X_train\", X_train_scaled), (\"y_train_cls\", y_train_cls), (\"y_train_reg\", y_train_reg),\n",
    "    (\"X_val\", X_val_scaled),     (\"y_val_cls\", y_val_cls),     (\"y_val_reg\", y_val_reg),\n",
    "    (\"X_test\", X_test_scaled),   (\"y_test_cls\", y_test_cls),   (\"y_test_reg\", y_test_reg),\n",
    "    (\"X_gen\", X_gen_scaled),     (\"y_gen_cls\", y_gen_cls),     (\"y_gen_reg\", y_gen_reg)\n",
    "]:\n",
    "    if not np.isfinite(arr).all():\n",
    "        raise ValueError(f\"{name} still contains NaN or Inf values!\")\n",
    "\n",
    "print(\"All features and targets are finite after cleaning.\")\n",
    "\n",
    "# 7. Save to disk\n",
    "# -----------------\n",
    "np.savez_compressed(\n",
    "    PROCESSED_DIR / f\"model_ready_{MAIN_STATION}.npz\",\n",
    "    X_train=X_train_scaled, y_train_cls=y_train_cls, y_train_reg=y_train_reg, dates_train=dates_train,\n",
    "    X_val=X_val_scaled,     y_val_cls=y_val_cls,     y_val_reg=y_val_reg,     dates_val=dates_val,\n",
    "    X_test=X_test_scaled,   y_test_cls=y_test_cls,   y_test_reg=y_test_reg,   dates_test=dates_test\n",
    ")\n",
    "np.savez_compressed(\n",
    "    PROCESSED_DIR / f\"model_ready_{GEN_STATION}.npz\",\n",
    "    X_gen=X_gen_scaled,     y_gen_cls=y_gen_cls,     y_gen_reg=y_gen_reg,     dates_gen=dates_gen\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09af6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
